{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import nltk\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, BATCH_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Sentiment Analysis Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential part of creating a sentiment analysis algorithm (or any data mining algorithm) is to use a comprehensive dataset or \"Corpus\" for learning, as well as a set of test data to ensure that the accuracy of the your algorithm meets the standards you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also allow you to adjust your algorithm in order to predict better (or more precisely) natural language features that you could extract from the text that will contribute to the sentiment classification, rather than using a generic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training dataset provided by the University of Michigan for Kaggle competitions: https://inclass.kaggle.com/c/si650winter11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 1,578,627 classified tweets and each row is marked as:\n",
    "\n",
    "- 1 for positive feeling\n",
    "- 0 for negative feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile('aux/datasets/sentiment-analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = file.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = file.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearLine(line):\n",
    "    columns = line.split(',')\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    \n",
    "    sentiment = columns[1]\n",
    "    tweet = columns[3].strip()\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    \n",
    "    tweetLower = [word.lower() for word in tweet]\n",
    "    \n",
    "    return(tweetLower, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTraining = dataset.map(lambda line: clearLine(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTraining.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = [[word, word + '_NEG'] for word in stopwords.words('english') for singleWord in word]\n",
    "stopWords = [word for wordPair in stopWords for word in wordPair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentAnalyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrainingSample = datasetTraining.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsNegative = sentimentAnalyzer.all_words([mark_negation(word) for word in datasetTrainingSample])\n",
    "wordsNegative = [word for word in wordsNegative if word not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramFeatures = sentimentAnalyzer.unigram_word_feats(wordsNegative, top_n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentAnalyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigramFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = sentimentAnalyzer.apply_features(datasetTrainingSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NaiveBayesClassifier.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sentimentAnalyzer.train(trainer, trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentence01 = [(['this', 'program', 'is', 'bad'], '')]\n",
    "testSentence02 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "testSentence03 = [(['that', 'place', 'is', 'awesome'], '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet01 = sentimentAnalyzer.apply_features(testSentence01)\n",
    "testSet02 = sentimentAnalyzer.apply_features(testSentence02)\n",
    "testSet03 = sentimentAnalyzer.apply_features(testSentence03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey = ''\n",
    "apiKeySecret = ''\n",
    "accessToken = ''\n",
    "accessTokenSecret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerm = 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlSample = 'https://stream.twitter.com/1.1/statuses/sample.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlFilter = 'https://stream.twitter.com/1.1/statuses/filter.json?track=' + searchTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests_oauthlib.OAuth1(apiKey, apiKeySecret, accessToken, accessTokenSecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stream Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = ssc.sparkContext.parallelize([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ssc.queueStream([], default = rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitterDataStream():\n",
    "    response = requests.get(urlFilter, auth = auth, stream = True)\n",
    "    print(urlFilter, response)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        try:\n",
    "            if count > NUM_TWEETS:\n",
    "                break\n",
    "\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['text']]\n",
    "            count += 1\n",
    "\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapData(t, rdd):\n",
    "    return rdd.flatMap(lambda x: twitterDataStream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(mapData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamCoord = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetClassifier(tweet):\n",
    "    sentence = [(tweet, '')]\n",
    "    testSet = sentimentAnalyzer.apply_features(sentence)\n",
    "    \n",
    "    print(tweet, classifier.classify(testSet[0][0]))\n",
    "    \n",
    "    return(tweet, classifier.classify(testSet[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetText(rdd):\n",
    "    for line in rdd:\n",
    "        tweet = line.strip()\n",
    "        translator = str.maketrans({key: None for key in string.punctuation})\n",
    "        tweet = tweet.translate(translator)\n",
    "        tweet = tweet.split(' ')\n",
    "        \n",
    "        tweetLower = [word.lower() for word in tweet]\n",
    "        \n",
    "        return tweetClassifier(tweetLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rddOutput(rdd):\n",
    "    global results\n",
    "    \n",
    "    pairs = rdd.map(lambda x: (getTweetText(x)[1], 1))\n",
    "    counts = pairs.reduceByKey(add)\n",
    "    output = [count for count in counts.collect()]\n",
    "    \n",
    "    result = [time.strftime('%I:%M:%S'), output]\n",
    "    results.append(result)\n",
    "    \n",
    "    print(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamCoord.foreachRDD(lambda t, rdd: rddOutput(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while count:\n",
    "    if len(results) > 5:\n",
    "        count = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFile = 'aux/' + time.strftime('%I%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddResults = sc.parallelize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddResults.saveAsTextFile(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddResults.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
